[
{
	"uri": "/",
	"title": "AWS RDS PostgreSQL",
	"tags": [],
	"description": "",
	"content": "About the AWS RDS PostgreSQL Foundation AWS RDS PostgreSQL foundation is essential for anyone who wants to use AWS RDS PostgreSQL to build and run database-driven applications. It is also useful for anyone who wants to learn more about PostgreSQL or relational databases in general.\nAWS RDS PostgreSQL foundation topics include: Introduction Database Upgrade Performance monitoring and optimization Backup and Recovery Learn more about PostgreSQL Clean Up Resources "
},
{
	"uri": "/4-backuprecovery/4-1-automatedbackups/",
	"title": "Automated Backups",
	"tags": [],
	"description": "",
	"content": "Reviewing your Instance\u0026rsquo;s Automatic Backups Let\u0026rsquo;s start by looking at the backups of the database you have created. A full database backup is taken immediately following database creation.\nOpen the Amazon RDS Console Click on rdspg-fcj-labs or the instance name you specified to reveal its details. Then select the Maintenance \u0026amp; backups tab.\nScroll down to the automated backup section and review the details. Note the backup window for the database and the latest possible restore time. Look at the available snapshots for the database. An automated snapshot from that initial database backup also appears.\n(OPTIONAL) AWS CLI Alternatively you can view the instance\u0026rsquo;s backups using the AWS CLI as shown below: AWS CLI\rAWSREGION=`aws configure get region`\r# List the automated backups for the instance\raws rds describe-db-instance-automated-backups \\\r--db-instance-identifier rdspg-fcj-labs \\\r--region $AWSREGION\r# List the snapshots for the instance\raws rds describe-db-snapshots \\\r--db-instance-identifier rdspg-fcj-labs \\\r--region $AWSREGION\r# Check the Latest Restorable Time (LRT) of the instance\raws rds describe-db-instances \\\r--db-instance-identifier rdspg-fcj-labs \\\r--query \u0026#39;DBInstances[].LatestRestorableTime\u0026#39; \\\r--region $AWSREGION \\\r--output text "
},
{
	"uri": "/2-databaseupgrade/2-1-automaticminorupgrade/",
	"title": "Automatic minor upgrade",
	"tags": [],
	"description": "",
	"content": "When you are creating the db instance you will find a checkbox that will enable automatic minor version upgrades like the following image.\nWe can find out if the option is enabled by running the following command:\nReplace your database \u0026amp; your region aws rds describe-db-instances --db-instance-identifier \u0026lt;your database name\u0026gt; --region \u0026lt;your region\u0026gt; --query \u0026#39;DBInstances[*].AutoMinorVersionUpgrade\u0026#39; Or by going to the RDS console, clicking on the db instance and select the Maintenance \u0026amp; backups tab.\nIn this case, you have not enabled Auto minor version upgrade in the first create database.\nNow, you can enable by click Modify button, then scroll down and select Enabled Auto minor version upgrade\nA PostgreSQL DB instance is automatically upgraded during your maintenance window if the following criteria are met:\nThe DB instance has the Auto minor version upgrade option enabled. The DB instance is running a minor DB engine version that is less than the current automatic upgrade minor version. "
},
{
	"uri": "/5-learnmoreaboutpostgresql/4-1-pgbench/",
	"title": "Benechmarking PostGreSQL Server",
	"tags": [],
	"description": "",
	"content": "Pgbench is a benchmarking tool that is included in the PostgreSQL distribution. It is used to test the performance of PostgreSQL servers under a variety of workloads. Pgbench can be used to test a wide range of factors, including:\nTransaction throughput Latency Memory usage CPU usage Disk I/O Pgbench works by creating a number of concurrent client sessions that execute a series of SQL statements. The SQL statements are typically representative of the types of transactions that are executed in a real-world application. Pgbench then measures the performance of the PostgreSQL server under the load generated by the client sessions.\nPgbench can be used to test a variety of workloads, including:\nTPC-B: A benchmark that simulates a complex online transaction processing (OLTP) workload. TPROC: A benchmark that simulates a simple OLTP workload. TPC-C: A benchmark that simulates a warehouse management system workload. Custom workloads: Pgbench can also be used to test custom workloads by specifying a script file that contains the SQL statements to be executed. To run pgbench, you simply need to specify the following parameters:\nThe number of concurrent client sessions The duration of the benchmark test The database to connect to The transaction script to use Pgbench will then generate a report that shows the performance of the PostgreSQL server under the load generated by the client sessions. The report will include the following metrics:\nTransactions per second (tps): The average number of transactions that were completed per second. Latency: The average time it took to complete a transaction. Memory usage: The amount of memory that was used by the PostgreSQL server during the benchmark. CPU usage: The amount of CPU time that was used by the PostgreSQL server during the benchmark. Disk I/O: The amount of disk I/O that was performed by the PostgreSQL server during the benchmark. Here are some tips for using pgbench for newbies:\nStart with a simple benchmark: If you are new to pgbench, it is best to start with a simple benchmark, such as the TPROC benchmark. This will help you to get familiar with the tool and to learn how to interpret the results. Use a small number of clients: It is also best to start with a small number of concurrent client sessions when running pgbench benchmarks. You can then gradually increase the number of clients to see how the performance of the PostgreSQL server changes under load. Monitor the performance of the PostgreSQL server: When running pgbench benchmarks, it is important to monitor the performance of the PostgreSQL server. You can use tools such as pgAdmin or pgBouncer to monitor metrics such as CPU usage, memory usage, and disk I/O. Analyze the results: Once you have run a pgbench benchmark, it is important to analyze the results. This will help you to identify any performance bottlenecks and to make changes to improve the performance of the PostgreSQL server. Now, We will explain in this workshop:\npgbench -i --fillfactor=90 --scale=500 --host=rdspg-fcj-labs.cssuddr073hp.us-east-1.rds.amazonaws.com --username masteruser pglab -i: *creates four tables pgbench_accounts, pgbench_branches, pgbench_history, and pgbench_tellers, destroying any existing tables of these names. -f (\u0026ndash;fillfactor=90): Specifies that the test data should be inserted using a fill factor of 90%. This means that each page of the database will be filled to 90% capacity before moving on to the next page. -s (\u0026ndash;scale=500): Specifies that the test data should be scaled to 500 times the size of the default test data set -h (\u0026ndash;host=rdspg-fcj-labs.cssuddr073hp.us-east-1.rds.amazonaws.com): Specifies the hostname of the PostgreSQL server to connect to. -U (\u0026ndash;username=masteruser): Specifies the username to use to connect to the PostgreSQL server. pglab: The name of the database to create and populate. pgbench --host=rdspg-fcj-labs.cssuddr073hp.us-east-1.rds.amazonaws.com --username masteruser --protocol=prepared -P 30 --time=300 --client=200 --jobs=200 pglab -h (\u0026ndash;host=rdspg-fcj-labs.cssuddr073hp.us-east-1.rds.amazonaws.com): Specifies the hostname of the PostgreSQL server to connect to. -U (\u0026ndash;username masteruser): Specifies the username to use to connect to the PostgreSQL server. -p (\u0026ndash;protocol=prepared): Specifies that prepared statements should be used to execute the benchmark workload. \u0026ndash;progress (-P 30): Specifies that the benchmark should run for 30 seconds. -T (\u0026ndash;time=300): Specifies that the benchmark should run for 300 seconds. -c (\u0026ndash;client=200): Specifies that 200 concurrent client sessions should be used to execute the benchmark workload. -j (\u0026ndash;jobs=200): Specifies that 200 jobs should be created to execute the benchmark workload. pglab: The name of the database to create and populate. "
},
{
	"uri": "/3-performancemonitoringandoptimization/3-1-cloudwatchlogsalerts/",
	"title": "CloudWatch Logs &amp; Alerts",
	"tags": [],
	"description": "",
	"content": "Viewing CloudWatch Logs When you created your database in the first lab you selected to publish logs to CloudWatch. Both the database and upgrade logs can be found in a CloudWatch Log Group associated with your database.\nAmazon CloudWatch allows us to consolidate logs from various AWS services into a single location to help draw correlations between services, like the application plane and Amazon RDS database.\nOpen CloudWatch Logs enter /aws/rds into the filter box and hit enter. Select the group assoicated with your database rds-pg-labs and take a look at the log stream.\nCreating an RDS Database Alarm Return to the Database List Click on rdspg-fcj-labs database to go to the details page. The Database detail page has a number of tabs you can look at.\nNow go to the Log \u0026amp; events tab and click on the Create alarm button.\nCreate a new alarm as shown below. Send notifications : choose Yes Send notifications to : choose New email or SMS topic Topic name : fill yourname topic AvgCPU-rdspg-fcj-labs With these recipients: fill your email address Metric : choose Average of CPUUtilization Threshold : choose \u0026gt;= 15 percent Evaluation period : default setting Name of alarm : default Now go to your email client for the email address you supplied for the notification. You should receive a confirmation email within a minute or two. Confirm your subscription by clicking the link in the email. To receive the alarm email that will be generated later in the lab, you will need to confirm your subscription. You should receive a confirmation email from Amazon SNS within 90 seconds or so. Click on the link to confirm your subscription. We will be triggering this alarm in a future section\nFinally, go to AWS SNS Subscriptions to check subscriptions status "
},
{
	"uri": "/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "What is AWS RDS PostgreSQL? AWS RDS PostgreSQL is a managed database service that makes it easy to set up, operate, and scale PostgreSQL databases in the cloud. RDS PostgreSQL handles all of the tasks involved in managing a PostgreSQL database, such as provisioning the hardware, configuring the database, and managing backups and restores.\nBenefits of using AWS RDS PostgreSQL There are many benefits to using AWS RDS PostgreSQL, including:\nEasy setup and management: RDS PostgreSQL takes care of all the tasks involved in setting up and managing a PostgreSQL database, such as provisioning the hardware, configuring the database, and managing backups and restores. This frees you up to focus on building and maintaining your applications.\nScalability: RDS PostgreSQL is highly scalable, making it easy to add or remove resources as needed. This is important for applications that need to handle spikes in traffic or that need to scale to meet growing demand.\nSecurity: RDS PostgreSQL provides a number of security features to protect your data, such as encryption at rest and in transit, access control lists, and auditing. This gives you the peace of mind knowing that your data is safe and secure.\nHigh availability: RDS PostgreSQL offers high availability, so you can be confident that your database will be available when you need it. RDS PostgreSQL provides features such as read replicas and automatic failover to ensure that your database is always available.\n"
},
{
	"uri": "/3-performancemonitoringandoptimization/3-2-createsomedbactivity/",
	"title": "Create some DB Activity",
	"tags": [],
	"description": "",
	"content": "Create some DB Activity Using MobaXterm to connect to your app server which you created at workshop 1\nThen run the following command to generate some activity on your RDS instance.\npgbench -i --fillfactor=90 --scale=500 --host=rdspg-fcj-labs.cssuddr073hp.us-east-1.rds.amazonaws.com --username masteruser pglab This will create a new database called pglab on the PostgreSQL server at rdspg-fcj-labs.cssuddr073hp.us-east-1.rds.amazonaws.com and populate it with 500 times the size of the default test data set. The test data will be inserted using a fill factor of 90%, which means that each page of the database will be filled to 90% capacity before moving on to the next page.\nOnce the pgbench command begins you can move on to the next session.\n"
},
{
	"uri": "/2-databaseupgrade/",
	"title": "Database Upgrade",
	"tags": [],
	"description": "",
	"content": "\rThis chapter assumes you have already created an RDS PostgreSQL instance in the first worshop.\nThis lab will take you through the upgrade process of an RDS PostgreSQL instance.\nThis lab contains following tasks: Automatic minor upgrade Upgrading the engine version Validate the upgrade process "
},
{
	"uri": "/4-backuprecovery/4-2-modifybackups/",
	"title": "Modify Backups",
	"tags": [],
	"description": "",
	"content": "Adjusting your backup window \u0026amp; backup retention Next we will change the backup window to 22:30 (UTC) and set the backup retention to 3 days.\nIn the upper right of the database details screen choose the Modify button\nOn the Mondify DB Instance page, scroll down to the backup section which is under \u0026ldquo;Additional Configuraiton\u0026rdquo; section. Using the pull down menu change the backup retention period to 3 days. The maximium retention period for automated backups is 35 days. Manual snapshots can be retained indefinetly\nUpdate the backup window to occur at 22:30 (UTC), so plan accordingly\nOnce you\u0026rsquo;ve made the appropriate changes click the continue button. Confirm the modifications on the next screen and choose the Apply immediately option. Click Modify DB Instance\nAs the changes are being applied you\u0026rsquo;ll be taken back to the database instance details page. Note: the exact start time of the backup will be chosen at random within the 30 minute window.\n(OPTIONAL) AWS CLI Alternatively you can modify the instance\u0026rsquo;s backup window using the AWS CLI as shown below:\nAWS CLI\rAWSREGION=`aws configure get region`\raws rds modify-db-instance \\\r--db-instance-identifier rds-pg-labs \\\r--preferred-backup-window 22:00-22:30 \\\r--apply-immediately \\\r--region $AWSREGION "
},
{
	"uri": "/2-databaseupgrade/2-2-upgradingtheengineversion/",
	"title": "Upgrading the engine version",
	"tags": [],
	"description": "",
	"content": "Minor version upgrades include only changes that are backward-compatible with existing applications.\nIf your PostgreSQL DB instance is using Read Replicas, you must upgrade all of the read replicas before upgrading the source instance. You could follow the same instructions below, but apply them first to read replicas.\nIf your DB instance is in a Multi-AZ deployment, both the writer and standby replicas are upgraded. Your DB instance might not be available until the upgrade is complete.\nLet\u0026rsquo;s upgrade our database instance now.\nIn the navigation pane, choose Databases, and then choose the DB instance that you want to upgrade.\nChoose Modify. The Modify DB Instance page appears.\nFor DB engine version, choose the new version. Choose Continue and check the summary of modifications. To apply the changes immediately, choose Apply immediately. Choosing this option can cause an outage in some cases.\nOn the confirmation page, review your changes. If they are correct, choose Modify DB Instance to save your changes.\nYou can see your instance being upgraded by going back to the RDS instances page.\n(OPTIONAL) AWS CLI Alternatively you can upgrade the instance using the AWS CLI as shown below:\nAWS CLI\rThe following command upgrades the instance to version 13.11.\naws rds modify-db-instance --db-instance-identifier \u0026lt;your database name\u0026gt; --engine-version 15.4 --apply-immediately --region \u0026lt;your region\u0026gt; "
},
{
	"uri": "/4-backuprecovery/4-3-manualsnapshots/",
	"title": "Manual Snapshots",
	"tags": [],
	"description": "",
	"content": "Taking a manual backup of your database. In addition to automated databsae backups, there are often times when you want to take an explicit backup of the database, just ahead of a major software release, or to refresh your staging enviornment. With AWS RDS these backups are called manual snapshots. RDS creates a storage volume snapshot of your DB instance, backing up the entire DB instance and not just individual databases. They are stored in Amazon S3 but they are not in a customer accessible bucket.\nWith your instance selected from the list of databases . Select Actions -\u0026gt; Take Snapshot\nOn the Take DB Snapshot screen, enter a name for your snapshot (e.g. manual-snapshot-rdspg-fcj-labs) and click Take Snapshot.\nAs the snapshot is creating, you will be taken to the Snapshots page in the AWS RDS Console. Let\u0026rsquo;s go to the list of databases and take a look at the instance status. You should see its backing up.\nOnce the database status returns to available return to the list of snapshots . Review all the snapshot details by scrolling to the right.\n(OPTIONAL) AWS CLI Alternatively you can take a manual snapshot of the instance using the AWS CLI as shown below:\nAWS CLI\rThe following command takes a manual snapshot of the instance.\nAWSREGION=`aws configure get region`\raws rds create-db-snapshot \\\r--db-instance-identifier rdspg-fcj-labs \\\r--db-snapshot-identifier manual-snapshot-rdspg-fcj-labs \\\r--region $AWSREGION "
},
{
	"uri": "/3-performancemonitoringandoptimization/",
	"title": "Performance monitoring",
	"tags": [],
	"description": "",
	"content": "\rThis chapter assumes you have already created an RDS PostgreSQL instance in the first worshop.\nMonitoring the health and preformance of your database is an important task. In this lab, you will configure an automated alert for one of your database performance metrics. Then you will run a generated workload against your Postgres database. From there you will view the performance metrics in the RDS Console and analyze the metrics using the RDS Performance Insights tool.\nAmazon RDS provides CloudWatch metrics for your database instances at no additional charge. You can use the RDS Management Console to view key operational metrics, including compute/memory/storage capacity utilization, I/O activity, and instance connections. Amazon RDS also provides Enhanced Monitoring, which provides access to over 50 metrics, including CPU, memory, file system, and disk I/O; and Performance Insights, an easy-to-use tool that helps you quickly detect performance problems.\nThis lab contains following tasks: CloudWatch Logs \u0026amp; Alerts Create some DB Activity Reviewing Performance Database Load Test Make your Load Test run faster "
},
{
	"uri": "/3-performancemonitoringandoptimization/3-3-reviewingperformance/",
	"title": "Reviewing Performance",
	"tags": [],
	"description": "",
	"content": "Now that we have a workload running against our AWS RDS PostgreSQL database, we can take a look at the metrics and dashboards available in CloudWatch and dive deeper with Performance Insights.\nIn the [RDS Console(https://console.aws.amazon.com/rds/home#databases)] , navigate to the Database instance details page for your database. To view CloudWatch metrics, click on the Monitoring tab. Explore various charts. You can click on the chart area of an individual chart to bring the chart full screen and get access to different chart customizations. To view Enhanced monitoring metrics, click on the Monitoring dropdown on the right side of the screen and pick Enhanced monitoring and explore various charts. Now look for the Performance Insights link in the RDS console. Right-click on the link and open up Performance Insights in a new browser tab. Select your database instance and you will notice the load being generated on your RDS instance Explore different waits in Performance Insights Next we will put a heavy load on the database and revisit these dashboards. "
},
{
	"uri": "/2-databaseupgrade/2-3-validatetheupgradeprocess/",
	"title": "Validate the upgrade process",
	"tags": [],
	"description": "",
	"content": "After a few minutes you will see that the database already finished the upgrade process and the status will be back to Available.\nClick on the instance, then the Configuration tab and you will see that the database engine version went from 15.3 to 15.4.\nOr if you rather use the CLI command, type in the console:\naws rds describe-db-instances --db-instance-identifier \u0026lt; your database name \u0026gt; --region \u0026lt; your region \u0026gt; --query DBInstances[*].EngineVersion You can see engine version after run command above:\n"
},
{
	"uri": "/4-backuprecovery/",
	"title": "Backup and Recovery",
	"tags": [],
	"description": "",
	"content": "\rThis chapter assumes have already created an RDS PostgreSQL instance in the first workshop.\nUnderstanding how your database is backed up and options for recovery are critically important before moving database workloads to the cloud. In this lab will take a look at the backup and recovery functions available in RDS for PostgreSQL.\nThis lab contains following tasks: Automated Backups Modify Backups Manual Snapshots Restore Snapshot Point in Time Restore AWS Backup "
},
{
	"uri": "/3-performancemonitoringandoptimization/3-4-databaseloadtest/",
	"title": "Database Load Test",
	"tags": [],
	"description": "",
	"content": "data load test Performance under stress Now let’s run a stress-test transactional workload on the RDS database instance. This workload will open up 200 connections to your database and each of those connections will continuously make updates to the tables. To launch the transactional workload, go back to the AWS CLI and run this command:\npgbench --host \u0026lt;your database endpoint\u0026gt;--username masteruser --protocol=prepared -P 30 --time=300 --client=200 --jobs=200 \u0026lt;your dabase name\u0026gt; This will start 200 concurrent client sessions that will execute the pgbench benchmark workload against the database specified by the parameter. The benchmark will run for 300 seconds, and the results will be printed to the terminal.\nLet\u0026rsquo;s revisit some of the dashboards we looked at in the earlier section.\nStart by looking at the monitoring tab for the database. You will see some variation in the data with the load test running, for instance you should now see a large increase in the number of database connections. Let\u0026rsquo;s also take a look at Performance Insights and explore the various charts as our 50 user stress-test runs.\nCounter metrics Database load Top SQL Notice that in the above example, there is a large sky blue area on the Database load graph. The sky blue area corresponds to a large number of database sessions that are waiting on the WALWriteLock \u0026amp; DataFileReadevent.\nFrom a performance optimization standpoint, if we can figure out how to reduce the number of sessions waiting on the WALWriteLock \u0026amp; DataFileRead event, our workload can run faster.\nYou can learn more about Performance Insights here .\nCheck your email. You should have received an email notification about the CPU Utilization alert you setup earlier in this lab. "
},
{
	"uri": "/5-learnmoreaboutpostgresql/",
	"title": "Learn more about RDS PostgreSQL",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/4-backuprecovery/4-4-restoresnapshot/",
	"title": "Restore Snapshot",
	"tags": [],
	"description": "",
	"content": "Restoring from manual snapshot Database backups are of very little value unless they can be used to restore the database. In this section we will take the manual snapshot just created and restore the PostgreSQL database.\nSelect the snapshot you created in the prior section from the list and select Actions, then click Restore Snapshot\nWhen restoring from a snapshot, a NEW RDS database instance is created, the original instance will continue to run normally\nComplete the Restore DB Instance page using the defaults except for the DB Instance Identifier where you can enter rdspg-fcj-labs-restore-manual-snapshot, Select db.t3.meidum as instance type and then select the Restore DB Instance at the bottom of the form.\nAs the restore initiates you are taken to the list of databases. You can monitor the status of the restoration and refresh the list until the restored database\u0026rsquo;s status is Available.\n(OPTIONAL) AWS CLI Alternatively you can restore an instance from a manual snapshot using the AWS CLI as shown below:\nAWS CLI\rThe following command takes a manual snapshot of the instance.\nAWSREGION=`aws configure get region`\raws rds restore-db-instance-from-db-snapshot \\\r--db-instance-identifier rdspg-fcj-labs-restore-manual-snapshot \\\r--db-snapshot-identifier manual-snapshot-rdspg-fcj-labs \\\r--db-instance-class db.t3.medium \\\r--region $AWSREGION "
},
{
	"uri": "/3-performancemonitoringandoptimization/3-5-makeyourloadtestrunfaster/",
	"title": "Make your Load Test run faster",
	"tags": [],
	"description": "",
	"content": "In this extra credit exercise, we will show you how to make your pgbench transactional workload faster. By faster, we mean we want to complete more transactions in the same amount of overall time. To do so, we will need to identify potential bottlenecks and fix them.\nFirst, let’s identify our current baseline showing the number of transactions per second our pgbench workload performed. Go to your MobaXterm window and look at how many tps (transactions per second) it reported for the 200 user pgbench load test you ran in the last module.\nIn the last two lines of the screenshot above, we see that our transactional workload was able to do approximately 578 transactions per second (tps). This is our baseline. The 578 tps was done on an db.t3.medium instance. If your lab environment uses a different instance type, such as the db.t3.medium, you may see a different value.\nNow, let\u0026rsquo;s analyze our performance. In general, you can look at database performance through both a micro and macro lens. At a micro level, you want to look at individual SQL statements and identify if any individual SQL statement is a candidate to be tuned. Performance Insights is a good tool for this kind of micro level analysis.\nAt a macro level, you want to look at performance metrics for the overall system. You do this to see if any of the key resources (such as CPU or Memory or IO) are being heavily taxed and becoming a bottleneck. The Monitoring metrics in the RDS Console (which are also available in AWS CloudWatch) are a good tool for this.\nLet\u0026rsquo;s begin by starting with the micro level and looking at our workload in Performance Insights. If we zoom into the time period of the workload (you can select a custom time period by dragging across a range of times in the Database Load chart), we can notice that our top wait event is WALWriteLock and DataFleRead. We identified this as our top wait event because we can visually see that it occupies the most area in our chart—in other words, it has the most number of sessions waiting on this event (an average of 100 database sessions are waiting on that event). We can also identify that the SQL Statement that is associated with the WALWriteLock is END, which in PostgreSQL is the statement that closes/commits the transaction.\nCounter metrics Database load Top SQL You may not be familiar with the WALWriteLock wait event. The WAL is the Write Ahead Log, also known as the transaction journal. These files contain a persistent record of COMMITs and COMMITs can not return to the client until the WAL information is safely written to disk.\nIn our case, it looks like a large number of our 200 database sessions are backing up on the WALWriteLock and these sessions are waiting on this event when their transaction ENDs (commits). In other words, the system appears that it is not able to write the commits to disk fast enough.\nLet\u0026rsquo;s now look at the macro level. To do so, navigate to the RDS Console and go to the Monitoring tab for your database. This will show us some system-wide metrics. Here is an example of what you will see: In the above chart, notice that CPUUtilization is not high even though we have 200 simultaneous connections on a 2 vCPU shape (db.t3.medium). So, we can say that it does not appear that CPU is a bottleneck.\nNext look at some of the I/O metrics. In the chart above, notice that our DiskQueueDepth of waiting I/O requests has gotten large. Also, notice that our Write IOPS is close to 1000. If you remember from when we created the database, we decided to use Provisioned IOPS (io1) storage with 1000 IOPS (if you forgot, you can see the current specification for your storage under the Configuration tab. Look for the Storage Type and IOPS fields). So, it is looking like we might be hitting the current limits of our allocated storage. As a final confirmation, you can also page through the charts and find the Write Latency chart:\nIn the WriteLatency chart, we can see that the write latencies got very high during our workload. This kind of behavior would be inline with our hypothesis that we are hitting the limits of our configured storage (1000 IOPS), which causes the queue depth and write latencies to increase, and which causes writes to the WAL to take longer, and which causes more and more database sessions to have to wait longer and longer for the WAL to be written when they commit their transaction.\nSo, it looks like we can make our transactional workload run faster if we increase the IOPS. The good news is that the storage used by RDS is elastic. You can increase/decrease the IOPS configuration while the database runs. You can even change the Storage Type (from io1 to gp2 or vice-versa) while the database runs if you wanted.\nLet’s modify our database and specify 3500 IOPS (up from its current value of 1000). To do so, click the Modify button at the top of the screen. On the modification screen, change the Provisioned IOPS field to 3500.\nThen scroll down to the bottom of the modification page and click Continue.\nOn the Summary and Scheduling page, be sure to choose Apply Immediately. Then click Modify DB Instance.\nAt this point, I suggest you go to the Databases List . The nice thing about the Databases List page is that it is easy to refresh the page so that you can track the changes in the Status. What you want to do is watch the Status while RDS changes the IOPS. The database will remain open while this happens. But we want to wait for the change to complete before we run our workload again. The process will take 10-15 minutes. You will see the status first say Modifying and then it will say Storage Optimization as the backend storage system optimizes itself to deliver on the new IOPS configuration.\n(OPTIONAL) AWS CLI Alternatively you can modify the IOPS of the instance using the AWS CLI as shown below:\nCommand\rThe following command modifies the IOPS of the instance.\naws rds modify-db-instance --db-instance-identifier rds-pg-labs --iops 3500 --allocated-storage 100 --apply-immediately --region \u0026lt; your region \u0026gt; Once the status returns to Available (or if you are impatient, wait at least until it says “Storage Optimization”), then return to Cloud9 and re-run the same workload as before:\npgbench --host \u0026lt;your database endpoint\u0026gt;--username masteruser --protocol=prepared -P 30 --time=300 --client=200 --jobs=200 \u0026lt;your dabase name\u0026gt; You should now see that the benchmark runs faster. For example, you should now see approximately 1300 transactions per second (up from 578 transactions per second before).\nSo, by increasing the IOPS capacity of the storage, you were able to make this stress test transactional workload run faster.\nYou are not limited to just using the RDS/CloudWatch metrics and Performance Insight. You can extend the default CloudWatch metrics with custom ones . You can use other PostgreSQL tools like the Dashboards built into pgAdmin. Or some PostgreSQL DBAs like to use the tool pgBadger to analyze database performance. To learn about how to use pgBadger with RDS PostgreSQL, read this .\nIn this workshop, We are using db.t3.mdedium instance type for handle labs. So, we might also see a large number of wait events for DataFileRead. The DataFileRead wait event could indicate that your instance type might not have enough memory to fit the working dataset into the database shared buffers. So, you could consider switching to an instance type with more memory as one possible way to address the DataFileRead waits.\n"
},
{
	"uri": "/4-backuprecovery/4-5-pointintimerestore/",
	"title": "Point in Time Restore",
	"tags": [],
	"description": "",
	"content": "Recover your database to a point in time Sometimes you will want to restore the database to a particular point in the past, just prior to running a data conversion script that encountered errors, or to refresh your stage environment to a state before an upgrade script was run. This is called a Point In Time Recovery or PITR.\nAmazon RDS for PostgreSQL allows restore to any point in time during your backup retention period. This is possible through the use of automated backups in combination with transaction logs, which are uploaded to S3 every 5 minutes.\nSo far in this section of the lab we have used the AWS Management Console for our tasks. You also can administer your RDS PostgreSQL instance from the command line using the AWS CLI. To demostrate this we will use the command-line interface for this particular lab.\nAmazon RDS keeps track of the latest restorable time for your database. We will lookup this information using the AWS-CLI. To run these commands we will use the Cloud9 enviornment you setup in the prerequisties section. If you haven\u0026rsquo;t done this, return here and complete this step.\nUsing your EC2 instances, lookup the latest restore time for your database.\naws rds describe-db-instances \\\r--db-instance-identifier rdspg-fcj-labs \\\r--region $AWSREGION \\\r--query \u0026#39;DBInstances[0].LatestRestorableTime\u0026#39; \\\r--output text Sample output below shows a latest restore time of Octorber 6, 2023 at 14:04 UTC\n2023-10-6T12:04:19Z Using the AWS-CLI we can use the following command to restore the database to the latest restorable time we looked up in the prior step. Be sure to update the time.\naws rds restore-db-instance-to-point-in-time \\\r--source-db-instance-identifier rdspg-fcj-labs \\\r--target-db-instance-identifier rdspg-fcj-labs-restore-latest \\\r--restore-time 2023-10-6T12:04:19Z You will see output similar to:\nOutput\r\u0026#34;DBInstance\u0026#34;: {\r\u0026#34;PubliclyAccessible\u0026#34;: true,\r\u0026#34;MasterUsername\u0026#34;: \u0026#34;masteruser\u0026#34;,\r\u0026#34;MonitoringInterval\u0026#34;: 0,\r\u0026#34;LicenseModel\u0026#34;: \u0026#34;postgresql-license\u0026#34;,\r\u0026#34;VpcSecurityGroups\u0026#34;: [\r{\r\u0026#34;Status\u0026#34;: \u0026#34;active\u0026#34;,\r\u0026#34;VpcSecurityGroupId\u0026#34;: \u0026#34;sg-040ccaea808501fad\u0026#34;\r}\r],\r\u0026#34;CopyTagsToSnapshot\u0026#34;: false,\r\u0026#34;OptionGroupMemberships\u0026#34;: [\r{\r\u0026#34;Status\u0026#34;: \u0026#34;pending-apply\u0026#34;,\r\u0026#34;OptionGroupName\u0026#34;: \u0026#34;default:postgres-15\u0026#34;\r}\r],\r\u0026#34;PendingModifiedValues\u0026#34;: {},\r\u0026#34;Engine\u0026#34;: \u0026#34;postgres\u0026#34;,\r\u0026#34;MultiAZ\u0026#34;: false,\r\u0026#34;DBSecurityGroups\u0026#34;: [],\r\u0026#34;DBParameterGroups\u0026#34;: [\r{\r\u0026#34;DBParameterGroupName\u0026#34;: \u0026#34;default.postgres15\u0026#34;,\r\u0026#34;ParameterApplyStatus\u0026#34;: \u0026#34;in-sync\u0026#34;\r}\r],\r\u0026#34;PerformanceInsightsEnabled\u0026#34;: false,\r\u0026#34;AutoMinorVersionUpgrade\u0026#34;: true,\r\u0026#34;PreferredBackupWindow\u0026#34;: \u0026#34;22:30-23:00\u0026#34;,\r\u0026#34;DBSubnetGroup\u0026#34;: {\r\u0026#34;Subnets\u0026#34;: [\r{\r\u0026#34;SubnetStatus\u0026#34;: \u0026#34;Active\u0026#34;,\r\u0026#34;SubnetIdentifier\u0026#34;: \u0026#34;subnet-0b17d2e34c1123500\u0026#34;,\r\u0026#34;SubnetOutpost\u0026#34;: {},\r\u0026#34;SubnetAvailabilityZone\u0026#34;: {\r\u0026#34;Name\u0026#34;: \u0026#34;us-east-1e\u0026#34;\r}\r},\r{\r\u0026#34;SubnetStatus\u0026#34;: \u0026#34;Active\u0026#34;,\r\u0026#34;SubnetIdentifier\u0026#34;: \u0026#34;subnet-0cf079d239ac29e77\u0026#34;,\r\u0026#34;SubnetOutpost\u0026#34;: {},\r\u0026#34;SubnetAvailabilityZone\u0026#34;: {\r\u0026#34;Name\u0026#34;: \u0026#34;us-east-1b\u0026#34;\r}\r},\r{\r\u0026#34;SubnetStatus\u0026#34;: \u0026#34;Active\u0026#34;,\r\u0026#34;SubnetIdentifier\u0026#34;: \u0026#34;subnet-0bee38e5233fa9f09\u0026#34;,\r\u0026#34;SubnetOutpost\u0026#34;: {},\r\u0026#34;SubnetAvailabilityZone\u0026#34;: {\r\u0026#34;Name\u0026#34;: \u0026#34;us-east-1f\u0026#34;\r}\r},\r{\r\u0026#34;SubnetStatus\u0026#34;: \u0026#34;Active\u0026#34;,\r\u0026#34;SubnetIdentifier\u0026#34;: \u0026#34;subnet-06327d061c2b4587b\u0026#34;,\r\u0026#34;SubnetOutpost\u0026#34;: {},\r\u0026#34;SubnetAvailabilityZone\u0026#34;: {\r\u0026#34;Name\u0026#34;: \u0026#34;us-east-1a\u0026#34;\r}\r},\r{\r\u0026#34;SubnetStatus\u0026#34;: \u0026#34;Active\u0026#34;,\r\u0026#34;SubnetIdentifier\u0026#34;: \u0026#34;subnet-0cd302225e2bf84ca\u0026#34;,\r\u0026#34;SubnetOutpost\u0026#34;: {},\r\u0026#34;SubnetAvailabilityZone\u0026#34;: {\r\u0026#34;Name\u0026#34;: \u0026#34;us-east-1d\u0026#34;\r}\r},\r{\r\u0026#34;SubnetStatus\u0026#34;: \u0026#34;Active\u0026#34;,\r\u0026#34;SubnetIdentifier\u0026#34;: \u0026#34;subnet-0b05e234776e32206\u0026#34;,\r\u0026#34;SubnetOutpost\u0026#34;: {},\r\u0026#34;SubnetAvailabilityZone\u0026#34;: {\r\u0026#34;Name\u0026#34;: \u0026#34;us-east-1c\u0026#34;\r}\r}\r],\r\u0026#34;DBSubnetGroupName\u0026#34;: \u0026#34;default\u0026#34;,\r\u0026#34;VpcId\u0026#34;: \u0026#34;vpc-05136ed012ff6cc5d\u0026#34;,\r\u0026#34;DBSubnetGroupDescription\u0026#34;: \u0026#34;default\u0026#34;,\r\u0026#34;SubnetGroupStatus\u0026#34;: \u0026#34;Complete\u0026#34;\r},\r\u0026#34;ReadReplicaDBInstanceIdentifiers\u0026#34;: [],\r\u0026#34;AllocatedStorage\u0026#34;: 100,\r\u0026#34;DBInstanceArn\u0026#34;: \u0026#34;arn:aws:rds:us-east-1:478371912360:db:rdspg-fcj-labs-restore-latest\u0026#34;,\r\u0026#34;BackupRetentionPeriod\u0026#34;: 3,\r\u0026#34;DBName\u0026#34;: \u0026#34;pglab\u0026#34;,\r\u0026#34;PreferredMaintenanceWindow\u0026#34;: \u0026#34;thu:08:05-thu:08:35\u0026#34;,\r\u0026#34;DBInstanceStatus\u0026#34;: \u0026#34;creating\u0026#34;,\r\u0026#34;IAMDatabaseAuthenticationEnabled\u0026#34;: false,\r\u0026#34;EngineVersion\u0026#34;: \u0026#34;15.3\u0026#34;,\r\u0026#34;MaxAllocatedStorage\u0026#34;: 1000,\r\u0026#34;DeletionProtection\u0026#34;: false,\r\u0026#34;DomainMemberships\u0026#34;: [],\r\u0026#34;StorageType\u0026#34;: \u0026#34;io1\u0026#34;,\r\u0026#34;DbiResourceId\u0026#34;: \u0026#34;db-YZM6TLDJKUBUM3MFXZIGZ5AGGM\u0026#34;,\r\u0026#34;CACertificateIdentifier\u0026#34;: \u0026#34;rds-ca-2019\u0026#34;,\r\u0026#34;Iops\u0026#34;: 1000,\r\u0026#34;StorageEncrypted\u0026#34;: true,\r\u0026#34;AssociatedRoles\u0026#34;: [],\r\u0026#34;KmsKeyId\u0026#34;: \u0026#34;arn:aws:kms:us-east-1:478371912360:key/dd420e91-002c-49bc-89d8-709cecce145a\u0026#34;,\r\u0026#34;DBInstanceClass\u0026#34;: \u0026#34;db.t3.medium\u0026#34;,\r\u0026#34;DbInstancePort\u0026#34;: 0,\r\u0026#34;DBInstanceIdentifier\u0026#34;: \u0026#34;rdspg-fcj-labs-restore-latest\u0026#34;\r} Now, let\u0026rsquo;s return to the RDS Console to check the restoring database. If you look at the details, note all the database specifications (e.g. DB Instance Class, Security Group) match the original database.\nNow we will use the RDS Console to restore our database to 30 minutes prior. Select the rdspg-fcj-labs database, choose Actions, and select Restore to point in time.\nOn the Launch DB Instance page, choose a custom and select a time 30 minutes prior. Enter a new DB instance identifier (e.g. rdspg-fcj-labs-earlier-restore), leave the remaining information at default values and click on Restore to point in time.\nAs the restore begins you will be take back to the list of databases and should see your new instance being created.\n"
},
{
	"uri": "/6-cleanup/",
	"title": "Clean up resources",
	"tags": [],
	"description": "",
	"content": "We will take the following steps to delete the resources we created in this exercise.\nDelete EC2 instance Go to EC2 service management console\nClick Instances. Select both Public Linux Instance and Private Windows Instance instances. Click Instance state. Click Terminate instance, then click Terminate to confirm. Go to IAM service management console\nClick Roles. In the search box, enter SSM. Click to select SSM-Role. Click Delete, then enter the role name SSM-Role and click Delete to delete the role. Click Users. Click on user Portfwd. Click Delete, then enter the user name Portfwd and click Delete to delete the user. Delete S3 bucket Access System Manager - Session Manager service management console.\nClick the Preferences tab. Click Edit. Scroll down. In the section S3 logging. Uncheck Enable to disable logging. Scroll down. Click Save. Go to S3 service management console\nClick on the S3 bucket we created for this lab. (Example: lab-fcj-bucket-0001 ) Click Empty. Enter permanently delete, then click Empty to proceed to delete the object in the bucket. Click Exit. After deleting all objects in the bucket, click Delete\nEnter the name of the S3 bucket, then click Delete bucket to proceed with deleting the S3 bucket. Delete VPC Endpoints Go to VPC service management console Click Endpoints. Select the 4 endpoints we created for the lab including SSM, SSMMESSAGES, EC2MESSAGES, S3GW. Click Actions. Click Delete VPC endpoints. In the confirm box, enter delete.\nClick Delete to proceed with deleting endpoints. Click the refresh icon, check that all endpoints have been deleted before proceeding to the next step.\nDelete VPC Go to VPC service management console\nClick Your VPCs. Click on Lab VPC. Click Actions. Click Delete VPC. In the confirm box, enter delete to confirm, click Delete to delete Lab VPC and related resources.\n"
},
{
	"uri": "/4-backuprecovery/4-6-awsbackup/",
	"title": "AWS Backup",
	"tags": [],
	"description": "",
	"content": "AWS Backup is a fully managed backup service that makes it easy to centralize and automate the backing up of data across AWS services. With AWS Backup, you can create backup policies called backup plans. You can use these plans to define your backup requirements, such as how frequently to back up your data and how long to retain those backups.\nAWS Backup lets you apply backup plans to your AWS resources by simply tagging them. AWS Backup will automatically backs up your AWS resources according to the backup plan that you defined.\nRDS/PostgreSQL will automatically backup your database and retain those backups for the length of your retention period, up to 35 days. Backups preformed via AWS Backup are considered manual snapshots, and will persist until deleted.\nCreating a Service Role for AWS Backup In order for AWS Backup to preform operations on your behalf we need to assign it a service role.\nFrom the IAM Console select Create role\nSelect AWS Service for the trusted entity type and use the use cases dropdown to find and select AWS Backup, select the AWS Backup radio button, then click Next.\nIn the add permissions step, use the filter by entering AWSBackupServiceRole and select the checkboxes for: AWSBackupServiceRolePolicyForBackup and AWSBackupServiceRolePolicyForRestore, then click Next.\nGive the role a name, rdspg-AWSBackupServiceRole, review the details then click Create Role.\nOn-Demand Backup from AWS Backup Begin in the AWS Backup Console . First create a Backup Vault, which is a logical container used to organize your backups. Click on Backup Vaults from the left-hand menu, then select Create Backup vault.\nGive the vault a name, rdspg-backup-vault and click Create Backup vault.\nWith your vault created you can now create an on-demand backup. Choose Protected resources from the left-hand menu, then click Create on-demand backup.\nComplete the dialog by selecting RDS and your resource type then choosing the rdspg-fcj-labs database. Select the backup vault you just created. Select choose an IAM role, and select your rdspg-AWSBackupServiceRole from the dropdown and finally hit Create on-demand backup.\nYou will see the backup in your backup jobs list. This is the same as the manual snapshot, but your backup is organized into a backup vault.\nSetup a Backup Plan in AWS Backup Selection of resources from a backup plan can be done using either resource tags or direct references.\nNow setup a backup plan using resource tags. Using AWS Backup this way will ensure that newly created resources that are properly tagged with be backed up via AWS Backup\nBegin by adding resource tags to your database from the RDS Console , click the rdspg-fcj-labs instance, then select the tags tab and choose Add.\nAdd Environment for production and ResourceType for rdspg-fcj tag to your rdspg-fcj-labs database.\nFrom the AWS Backup Console select Backup plans from the left-hand menu, then choose Create backup plan.\nSelect Build new plan, enter Backup plan name rdspg-backup-plan, enter Backup rule name DailyBackups, select Backup vault rdspg-backup-vault, leaving everything else at the default, then click Create plan.\nComplete the dialog by entering the resource assignment name, rdspg-bp-resource-selection, choose IAM role and select the IAM role created earlier rdspg-AWSBackupServiceRole. Finally add the Environment and ResoureType tags as shown in the screengrab.\nCongratulations! Now that you have created a backup plan based on tags, any databases you create in future with these tags will be automatically backed up with this plan.\n"
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]